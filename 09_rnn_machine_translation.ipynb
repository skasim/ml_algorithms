{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4f31fe-1162-4513-b18a-69367822a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm --quiet\n",
    "# ! python -m spacy download de_core_news_sm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73cdff1-cc8b-4b7c-8c43-ed9658e27d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# references: \n",
    "# https://hussainwali.medium.com/transforming-your-text-data-with-pytorch-12ec1b1c9ae6\n",
    "# https://github.com/bentrevett/pytorch-seq2seq/tree/main\n",
    "# https://adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-recurrent-neural-network-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e794a1-f25a-47bf-abaa-39ae4a3e28ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samra/anaconda3/envs/ml_demos/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e459ef-a4fc-4555-95b2-0b272bc1ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")\n",
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a354be-6ddf-4068-b45b-9d7cb3099e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73bff59-9f19-4a04-ad2c-3e53828b460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n",
    "    en_tokens = [token.lower() for token in en_tokens]\n",
    "    de_tokens = [token.lower() for token in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b45c10c-c88a-4db8-94da-c85a00a2741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████| 1000/1000 [00:00<00:00, 2139.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"de_nlp\": de_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d38cbc-f286-4441-98ad-885f258dea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    "    max_tokens=1_000\n",
    ")\n",
    "\n",
    "de_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"de_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    "    max_tokens=1_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700f8be0-39ad-4cc0-b300-b4f92191d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert en_vocab[unk_token] == de_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == de_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]\n",
    "\n",
    "en_vocab.set_default_index(unk_index)\n",
    "de_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b2030c1-586f-4ba3-be7c-879072882ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, en_vocab, de_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "361b4ba5-d07b-452d-acdd-3cf2b71b7759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████| 1000/1000 [00:00<00:00, 6448.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0809c225-0fba-4770-a0b8-c4dcee8dbb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_ids': tensor([  2,  16,  24,  15,  25, 778,  17,  57,  80, 202,   0,   5,   3]),\n",
       " 'de_ids': tensor([  2,  18,  26, 253,  30,  84,  20,  88,   7,  15, 110,   0,   0,   4,\n",
       "           3]),\n",
       " 'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91874950-ae8f-4322-a033-0e3d98a86173",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1c2f4ff-135c-4485-ac99-c363fa634fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_ids': tensor([  2,  16,  24,  15,  25, 778,  17,  57,  80, 202,   0,   5,   3]),\n",
       " 'de_ids': tensor([  2,  18,  26, 253,  30,  84,  20,  88,   7,  15, 110,   0,   0,   4,\n",
       "           3]),\n",
       " 'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87c6512c-dc56-4c14-a855-19adcc78e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b707ce51-93b6-4dc8-b39e-a72e6bde5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6b8a7b0-3470-425d-af81-555ba50d0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_dataloader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_dataloader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8351a150-3d0e-4ae7-a03a-ecf2b27a84a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataloader)[7][\"en_ids\"].shape\n",
    "# batch is of shape seq_len, batch_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6630892-eabe-4d51-81ae-8b16dec04787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN implementation: https://aclanthology.org/attachments/D14-1179.Attachment.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bef87a30-266e-4f9a-abf1-593e42cebda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_sz, embedding_dim, hidden_dim, n_layers, p_dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_sz, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "    def forward(self, src):\n",
    "        # input is of shape [seq_len, batch_sz]\n",
    "        embeddings = self.dropout(self.embedding(src))\n",
    "        # out is of shape [seq_len, batch_sz, n_directions * hidden_dim]\n",
    "        # hidden is of shape [n_directions * n_layers, batch_sz, hidden_dim]\n",
    "        out, hidden = self.gru(embeddings)\n",
    "        return out, hidden\n",
    "\n",
    "# wee test\n",
    "voc_sz = 2\n",
    "b_sz = 3\n",
    "seq_l = 4\n",
    "n_lrs = 1\n",
    "x = torch.randint(0, 2, (seq_l, b_sz))\n",
    "# input is [4, 3]\n",
    "enc = Encoder(voc_sz, b_sz, seq_l, n_lrs)\n",
    "o, h = enc(x)\n",
    "assert list(o.shape) == [4, 3, 4]\n",
    "assert list(h.shape) == [1, 3, 4]\n",
    "\n",
    "# Example parameters\n",
    "vocab_size = 50  # Size of vocabulary\n",
    "embedding_dim = 16  # Size of embeddings\n",
    "hidden_size = 32  # Size of hidden state\n",
    "seq_len = 10\n",
    "batch_size = 4\n",
    "\n",
    "# Random input tokens (integer indices)\n",
    "input_tokens = torch.randint(0, vocab_size, (seq_len, batch_size))\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = Encoder(vocab_size, embedding_dim, hidden_size)\n",
    "\n",
    "# Forward pass\n",
    "hidden_states, final_hidden_state = encoder(input_tokens)\n",
    "\n",
    "# print(f\"Hidden states shape: {hidden_states.shape}\")\n",
    "print(f\"Final hidden state shape: {final_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "c1b2de3f-89e8-4ff7-94fa-c094d4a1dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOneStep(nn.Module):\n",
    "    def __init__(self, input_output_dim, embedding_dim, hidden_dim, n_layers, p_dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.input_output_dim = input_output_dim\n",
    "        self.embedding = nn.Embedding(input_output_dim, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, input_output_dim)\n",
    "    def forward(self, trg, hidden):\n",
    "        # since we get only one token at a time, \n",
    "        # it is of shape [batch_sz]\n",
    "        # so we need to unsqueeze to make it [1, batch_sz]\n",
    "        trg = trg.unsqueeze(0)\n",
    "        embeddings = self.dropout(self.embedding(trg))\n",
    "        out, hidden = self.gru(embeddings, hidden)\n",
    "        # out is of shape [seq_len, batch_sz, n_directions * hidden_dim]\n",
    "        # hidden is of shape [n_directions * n_layers, batch_sz, hidden_dim]\n",
    "        # but input for fc has to be squeezed at 0-th dim to remove the dimension added earlier\n",
    "        # so that shape of out is [batch_sz, hidden_dim]\n",
    "        out = out.squeeze(0)\n",
    "        out = self.fc(out)\n",
    "        # out here is [batch_sz, input_output_dim]\n",
    "        return out, hidden\n",
    "\n",
    "# wee test\n",
    "io_dim = 5\n",
    "emb_dim = 10\n",
    "h_dim = 4\n",
    "n_lrs = 1\n",
    "d1s = DecoderOneStep(io_dim, emb_dim, h_dim, n_lrs) \n",
    "single_step_input = torch.tensor([1,2,2,1])\n",
    "h = torch.randn(1, 4, h_dim) # batch_sz is 4 because the input into single step is [seq_len, batch_sz]\n",
    "print(h.shape)\n",
    "o1s, h1s = d1s(single_step_input, h)\n",
    "# assert list(o1s.shape) == [4, 5]\n",
    "assert list(h1s.shape) == [1, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "644148dc-978b-48f6-9fbf-a30faf46794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, one_step_decoder):\n",
    "        super().__init__()\n",
    "        self.one_step_decoder = one_step_decoder\n",
    "    def forward(self, trg, hidden, teacher_forcing_ratio=0.5):\n",
    "        seq_len, batch_sz = trg.shape\n",
    "        vocab_sz = self.one_step_decoder.input_output_dim\n",
    "        predictions = torch.zeros(seq_len, batch_sz, vocab_sz)\n",
    "        input_ = trg[0, :]\n",
    "        for t in range(seq_len):\n",
    "            pred, hidden = self.one_step_decoder(input_, hidden)\n",
    "            predictions[t] = pred\n",
    "            is_teacher_force =  random.random() < teacher_forcing_ratio \n",
    "            input_ = trg[t] if is_teacher_force else pred.argmax(1)\n",
    "        # predictions are of shape [seq_len, batch_sz, input_output_dim]\n",
    "        return predictions\n",
    "\n",
    "# wee test\n",
    "dec = Decoder(d1s)\n",
    "y = torch.randint(0, 2, (seq_l, b_sz))\n",
    "odec = dec(y, h)\n",
    "assert list(odec.shape) == [seq_l, b_sz, io_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "6edd0544-30f1-4d53-aa2a-3d5d557c5f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDec(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        _, context = self.encoder(src)\n",
    "        predictions = self.decoder(trg, context, teacher_forcing_ratio)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "5134e8e9-5710-47e8-96d8-21a6ba7a6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(en_vocab, de_vocab, pad_index):\n",
    "    # params\n",
    "    input_dim = len(en_vocab)\n",
    "    output_dim = len(de_vocab)\n",
    "    encoder_embedding_dim = 256\n",
    "    decoder_embedding_dim = 256\n",
    "    hidden_dim = 512\n",
    "    encoder_dropout = 0.5\n",
    "    decoder_dropout = 0.5\n",
    "    n_layers = 1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # init models\n",
    "    encoder = Encoder(input_dim, encoder_embedding_dim, hidden_dim, n_layers, encoder_dropout)\n",
    "    decoder_one_step = DecoderOneStep(output_dim, decoder_embedding_dim, hidden_dim, n_layers, decoder_dropout)\n",
    "    decoder = Decoder(decoder_one_step)\n",
    "    seq2seq = EncDec(encoder, decoder)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.Adam(seq2seq.parameters())\n",
    "    \n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "    \n",
    "    return seq2seq, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "f6d14ad2-e4f1-4ca7-8428-ebd564f08712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, en_vocab, de_vocab, pad_index, n_epochs=3):\n",
    "    model, optimizer, criterion = init_model(en_vocab, de_vocab, pad_index)\n",
    "    clip = 1\n",
    "    # training\n",
    "    for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "        training_losses = []\n",
    "        model.train()\n",
    "        \n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            src = batch[\"en_ids\"]\n",
    "            trg = batch[\"de_ids\"]\n",
    "            # src shape [src_seq_len, batch_sz]\n",
    "            # trg shape [trg_seq_len, batch_sz]\n",
    "\n",
    "            # forward pass\n",
    "            y_pred = model(src, trg, teacher_forcing_ratio=0.5)\n",
    "            # y_pred shape [trg_seq_len, batch_sz, trg_vocab_sz]\n",
    "            trg_vocab_sz = y_pred.shape[-1]\n",
    "            # trg_vocab_sz shape is just len of trg vocab\n",
    "            # discard first token from output\n",
    "            y_pred = y_pred[1:].view(-1, trg_vocab_sz) # means do whatever you want with other dims and last dim has to be trg_vocab_sz\n",
    "            # or y_pred after discarding first token \n",
    "            # shape is [trg_seq_len - 1, batch_sz, vocab_sz]\n",
    "            # .view(-1, trg_vocab_sz returns shape\n",
    "            # [(trg_seq_len - 1) * batch_sz, vocab_sz]\n",
    "            # so now trg needs to be updated same as above\n",
    "            trg = trg[1:].view(-1) # not sure why this can be [(trg_seq_len - 1) * batch_sz]\n",
    "\n",
    "            # calc loss\n",
    "            loss = criterion(y_pred, trg)\n",
    "\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            training_losses.append(loss.item())\n",
    "        print(f\"epoch {epoch} average training loss: {sum(training_losses) / len(training_losses)}\")\n",
    "    \n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            validation_losses = []\n",
    "            for i, batch in enumerate(val_dataloader):\n",
    "                src = batch[\"en_ids\"]\n",
    "                trg = batch[\"de_ids\"]\n",
    "    \n",
    "                # forward pass\n",
    "                y_pred = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "                trg_vocab_sz = y_pred.shape[-1]\n",
    "                y_pred = y_pred[1:].view(-1, trg_vocab_sz)\n",
    "                trg = trg[1:].view(-1)\n",
    "    \n",
    "                # calc loss\n",
    "                loss = criterion(y_pred, trg)\n",
    "    \n",
    "                validation_losses.append(loss.item())\n",
    "            print(f\"epoch {epoch} average validation loss: {sum(validation_losses) / len(validation_losses)}\")   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "05e4dcc4-81e4-4bbd-9611-05ff001b836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                           | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 average training loss: 3.8569061798146116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████▋                                 | 1/3 [04:49<09:39, 289.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 average validation loss: 3.817979872226715\n",
      "epoch 1 average training loss: 3.149116719871891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████████▎                | 2/3 [10:06<05:05, 305.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 average validation loss: 3.5690895318984985\n",
      "epoch 2 average training loss: 2.8717597371155996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 3/3 [13:46<00:00, 275.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 average validation loss: 3.358761876821518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncDec(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(998, 256)\n",
       "    (gru): GRU(256, 512)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (one_step_decoder): DecoderOneStep(\n",
       "      (embedding): Embedding(998, 256)\n",
       "      (gru): GRU(256, 512)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (fc): Linear(in_features=512, out_features=998, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model = train(train_dataloader, valid_dataloader, en_vocab, de_vocab, pad_index)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "d2d2636b-d3ea-496e-9daa-32497938c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,388,390 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "f0cfed5c-5714-47d4-83a8-523d0bc5d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        tokens = [token.text for token in en_nlp.tokenizer(sentence)]\n",
    "        print(tokens)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        ids = en_vocab.lookup_indices(tokens)\n",
    "        print(f\"in tokens: {ids}\")\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        _, context = model.encoder(tensor)\n",
    "        hidden = context\n",
    "        next_token = de_vocab.lookup_indices([sos_token])\n",
    "        next_token = next_token[0]\n",
    "        outputs = []\n",
    "        for _ in range(max_output_length):\n",
    "            next_token = torch.LongTensor([next_token])\n",
    "            output, hidden = model.decoder.one_step_decoder(next_token, hidden)\n",
    "            next_token = output.argmax(1).item()\n",
    "            if next_token == de_vocab[eos_token]:\n",
    "                break\n",
    "            else:\n",
    "                outputs.append(next_token)\n",
    "        print(f\"out tokens: {outputs}\")\n",
    "        tokens = de_vocab.lookup_tokens(outputs)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "88c4db22-ee70-4806-b719-8c3b723111e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'black', 'dog', 'and', 'a', 'brown', 'dog', 'with', 'a', 'ball', '.']\n",
      "in tokens: [2, 4, 26, 35, 11, 4, 61, 35, 13, 4, 68, 5, 3]\n",
      "out tokens: [5, 5, 114, 32, 11, 6, 293, 11, 6, 293, 4]\n",
      "\n",
      "sentence: A black dog and a brown dog with a ball.\n",
      "\n",
      "expected_translation: Ein schwarzer und ein brauner Hund mit einem Ball.\n",
      "\n",
      "actual translation: ein schwarzer hund mit einem stock mit einem stock .\n"
     ]
    }
   ],
   "source": [
    "test_sentences = test_data\n",
    "rando_idx = np.random.randint(low=0, high=len(test_sentences))\n",
    "sentence = test_sentences[rando_idx][\"en\"]\n",
    "expected_translation = test_sentences[rando_idx][\"de\"]\n",
    "translation = translate_sentence(\n",
    "    sentence=sentence,\n",
    "    model=model,\n",
    "    en_nlp=en_nlp,\n",
    "    de_nlp=de_nlp,\n",
    "    en_vocab=en_vocab,\n",
    "    de_vocab=de_vocab,\n",
    "    sos_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "print(f\"\\nsentence: {sentence}\\n\")\n",
    "print(f\"expected_translation: {expected_translation}\\n\")\n",
    "print(f\"actual translation: {' '.join(i for i in translation[1:])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_demos",
   "language": "python",
   "name": "ml_demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
