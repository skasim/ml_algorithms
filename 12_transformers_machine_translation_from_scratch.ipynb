{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4f31fe-1162-4513-b18a-69367822a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm --quiet\n",
    "# ! python -m spacy download de_core_news_sm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e794a1-f25a-47bf-abaa-39ae4a3e28ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samra/anaconda3/envs/ml_demos/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import datasets\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e459ef-a4fc-4555-95b2-0b272bc1ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")\n",
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a354be-6ddf-4068-b45b-9d7cb3099e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73bff59-9f19-4a04-ad2c-3e53828b460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n",
    "    en_tokens = [token.lower() for token in en_tokens]\n",
    "    de_tokens = [token.lower() for token in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45c10c-c88a-4db8-94da-c85a00a2741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"de_nlp\": de_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d38cbc-f286-4441-98ad-885f258dea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    "    max_tokens=1_000\n",
    ")\n",
    "\n",
    "de_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"de_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    "    max_tokens=1_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f8be0-39ad-4cc0-b300-b4f92191d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert en_vocab[unk_token] == de_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == de_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]\n",
    "\n",
    "en_vocab.set_default_index(unk_index)\n",
    "de_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2030c1-586f-4ba3-be7c-879072882ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, en_vocab, de_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b4ba5-d07b-452d-acdd-3cf2b71b7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0809c225-0fba-4770-a0b8-c4dcee8dbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91874950-ae8f-4322-a033-0e3d98a86173",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c2f4ff-135c-4485-ac99-c363fa634fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6512c-dc56-4c14-a855-19adcc78e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index, batch_first=True)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index, batch_first=True)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707ce51-93b6-4dc8-b39e-a72e6bde5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8a7b0-3470-425d-af81-555ba50d0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_dataloader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_dataloader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8351a150-3d0e-4ae7-a03a-ecf2b27a84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_dataloader)[7][\"en_ids\"].shape\n",
    "# batch is of shape batch_sz, seq_len\n",
    "# has to be updated because transformers take in a \"sentence\" at a time as they need to pay attention to all\n",
    "# the tokens in the sentence at once, so instead of seq_len, batch_sz, the data needs to be batch_sz, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c87c1-784b-4fa5-a6f4-e8a9c4634717",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_dataloader)[7][\"en_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6630892-eabe-4d51-81ae-8b16dec04787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers (attention is all you need) implementation: https://arxiv.org/pdf/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091b106-55c7-4df0-9004-aedf443ca66e",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d257e-3840-4743-8a3f-b13fc73aadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, max_seq_len, emb_dim, p_dropout=0.1):\n",
    "        # max_seq_len is number of tokens in a sentence\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.emb_dim = emb_dim\n",
    "        # emb_dim is the dims of one token\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        # to prevent raising large bases via an exp, use the property a^b = exp(b * ln(a))\n",
    "        # then you have 2i * ln(10,000) / dmodel and taking negative of the exponent results in the inervse\n",
    "        # e.g., 5 ^ 2 = 10 and 5 ^ -2 = 1/10\n",
    "        divisor = torch.exp(torch.arange(0, emb_dim, 2).float() * (-math.log(10_000.0) / emb_dim))  # i.e., exp(2i * -ln(10,000) / dmodel)\n",
    "        # apply sin only to the even columns\n",
    "        pe[:, 0::2] = torch.sin(pos * divisor)\n",
    "        # apply cos only to the odd columns\n",
    "        pe[:, 1::2] = torch.cos(pos * divisor)\n",
    "        pe = pe.unsqueeze(0) # pe: [1, max_seq_len, emb_dim]\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        # pe_batch = self.pe[:, :seq_len].clone().detach()\n",
    "        # return self.dropout(x + pe_batch) [batch_sz, seq_len, emb_dim]\n",
    "        return self.dropout(x + self.pe[:, :seq_len, :]) # [batch_sz, seq_len, emb_dim]\n",
    "        \n",
    "# wee test\n",
    "batch_sz = 3\n",
    "seq_len = 5\n",
    "emb_dim = 6\n",
    "pos_enc = PositionalEncoder(seq_len, emb_dim)\n",
    "x_test = torch.randn(batch_sz, seq_len, emb_dim)\n",
    "o = pos_enc(x_test)\n",
    "assert list(o.shape) == [batch_sz, seq_len, emb_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f0d95-6f0b-4508-9379-573576a77b4c",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690795c-63b4-46ae-ac09-0c8774711c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_head):\n",
    "        super().__init__()\n",
    "        self.dim_head = torch.tensor(dim_head)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v: [batch_sz, n_heads, seq_len, dim_head]\n",
    "        k = k.transpose(-2, -1) # key: [batch_sz, n_heads, dim_head, seq_len] for batch key.T\n",
    "        attn = q @ k # attn: [batch_sz, n_heads, seq_len, seq_len]\n",
    "        # attn is a comparison of every word in the sentence against every other word in the sentence. hence, a square matrix\n",
    "        attn = attn / torch.sqrt(self.dim_head)\n",
    "        # masking\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask==0, float(\"-inf\"))\n",
    "        attn = torch.softmax(attn, dim=-1) # attn: [batch_sz, n_head, seq_len, seq_len]\n",
    "        attn = attn @ v # attn: [batch_sz, n_head, seq_len, dim_head]\n",
    "        return attn\n",
    "\n",
    "# wee test\n",
    "dmodel = 6\n",
    "bsize = 5\n",
    "seq_len = 4\n",
    "emb_dim = 3\n",
    "n_heads = 2\n",
    "input_test = torch.randn(bsize, n_heads, seq_len, emb_dim)\n",
    "attn_test = Attention(dmodel)\n",
    "o = attn_test(input_test, input_test, input_test)\n",
    "assert list(o.shape) == [5, 2, 4, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9111c0-d0c7-4b15-b3a4-dead94ad09ec",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c12be5-66ac-4bac-8892-ffcbf450d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads= num_heads\n",
    "        assert emb_dim % num_heads == 0\n",
    "        self.dim_head = emb_dim // num_heads\n",
    "        self.attention = Attention(emb_dim)\n",
    "        self.fc_out = nn.Linear(emb_dim, emb_dim)\n",
    "    \n",
    "    def split_input(self, x):\n",
    "        batch_sz, seq_len, emb_dim = x.shape\n",
    "        split = x.view(batch_sz, seq_len, self.num_heads, self.dim_head) # split: [batch_sz, seq_len, num_heads, dim_head]\n",
    "        return split.permute(0, 2, 1, 3) # split: [batch_sz, num_heads, seq_len, dim_head]\n",
    "\n",
    "    def cat_heads(self, x):\n",
    "        batch_sz, num_heads, seq_len, head_dim = x.shape\n",
    "        x = torch.transpose(x, 2, 1)\n",
    "        x = torch.reshape(x, (batch_sz, seq_len, head_dim * num_heads))\n",
    "        return x # x: [batch_sz, seq_len, dim_head * num_heads]\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q = self.split_input(q) # [batch_sz, num_heads, seq_len, dim_head]\n",
    "        k = self.split_input(k) # [batch_sz, num_heads, seq_len, dim_head]\n",
    "        v = self.split_input(v) # [batch_sz, num_heads, seq_len, dim_head]\n",
    "        attn = self.attention(q, k, v, mask) # [batch_sz, num_heads, seq_len, dim_head]\n",
    "        attn = self.cat_heads(attn) # [batch_sz, seq_len, dim_head * num_heads]\n",
    "        attn = self.fc_out(attn) # \n",
    "        return attn # attn: [batch_sz, seq_len, emb_dim]\n",
    "        \n",
    "# wee test\n",
    "bsize = 5\n",
    "seq_len = 3\n",
    "head_dim = 16\n",
    "# out_emb_dim in this example is 4\n",
    "input_test = torch.randn(bsize, seq_len, head_dim)\n",
    "mha = MultiHeadAttention(head_dim)\n",
    "o = mha(input_test, input_test, input_test)\n",
    "assert list(o.shape) == [5, 3, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b3e6a-6b87-40d7-9152-a2c16ab99bca",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65681e58-8678-48e0-b7fa-4fd10ceead6c",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01411243-48fe-437f-bfe3-a509177b220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder1L(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(emb_dim, emb_dim) \n",
    "        self.K = nn.Linear(emb_dim, emb_dim)\n",
    "        self.V = nn.Linear(emb_dim, emb_dim)\n",
    "        self.mhead_attn = MultiHeadAttention(emb_dim, n_heads)\n",
    "        self.feed_fwd = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim))\n",
    "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.dropout1 = nn.Dropout(p_dropout)\n",
    "        self.dropout2 = nn.Dropout(p_dropout)\n",
    "    def forward(self, x, mask):\n",
    "        # x: [batch_sz, seq_ln, emb_dim]\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        x = self.mhead_attn(q, k, v, mask)\n",
    "        x = self.dropout1(x) + x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.feed_fwd(x)\n",
    "        x = self.dropout2(x) + x\n",
    "        x = self.layer_norm2(x)\n",
    "        return x #  x: [batch_sz, seq_ln, emb_dim]\n",
    "\n",
    "# wee test\n",
    "bsize = 5\n",
    "seq_len = 3\n",
    "emb_dim = 16\n",
    "n_heads = 4\n",
    "test_x = torch.randn(bsize, seq_len, emb_dim)\n",
    "e1l = Encoder1L(emb_dim, n_heads)\n",
    "o = e1l(test_x, mask=None)\n",
    "assert list(o.shape) == [5, 3, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080260a2-3b8c-4763-8cb6-3ec294523a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_vocab_sz, max_seq_len, emb_dim, n_layers, n_attn_heads, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc = nn.ModuleList([Encoder1L(emb_dim, n_attn_heads) for i in range(n_layers)])\n",
    "        self.input_embeddings = nn.Embedding(in_vocab_sz, emb_dim)\n",
    "        self.pos_enc = PositionalEncoder(max_seq_len, emb_dim)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_embeddings(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.enc[i](x, mask)\n",
    "        return x\n",
    "\n",
    "# wee test\n",
    "bsize = 5\n",
    "max_seq_len = 3\n",
    "emb_dim = 16\n",
    "n_layers = 2\n",
    "n_heads = 4\n",
    "in_voc = 20\n",
    "test_x = torch.randint(low=0, high=10, size=(bsize, max_seq_len))\n",
    "enc = Encoder(in_voc, max_seq_len, emb_dim, n_layers, n_heads)\n",
    "o = enc(test_x, mask=None)\n",
    "assert list(o.shape) == [5, 3, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ef1bc-92b9-4da4-974c-0056b8393a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder1L(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.Q1 = nn.Linear(emb_dim, emb_dim) \n",
    "        self.K1 = nn.Linear(emb_dim, emb_dim)\n",
    "        self.V1 = nn.Linear(emb_dim, emb_dim)\n",
    "        self.Q2 = nn.Linear(emb_dim, emb_dim) \n",
    "        self.K2 = nn.Linear(emb_dim, emb_dim)\n",
    "        self.V2 = nn.Linear(emb_dim, emb_dim)\n",
    "        self.mhead_attn = MultiHeadAttention(emb_dim, n_heads)\n",
    "        self.masked_mhead_attn = MultiHeadAttention(emb_dim, n_heads)\n",
    "        self.feed_fwd = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim))\n",
    "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(emb_dim)\n",
    "        self.dropout1 = nn.Dropout(p_dropout)\n",
    "        self.dropout2 = nn.Dropout(p_dropout)\n",
    "        self.dropout3 = nn.Dropout(p_dropout)\n",
    "    def forward(self, enc_x, x, src_mask, trg_mask):\n",
    "        q1 = self.Q1(x)\n",
    "        k1 = self.K1(x)\n",
    "        v1 = self.V1(x)\n",
    "        x = self.masked_mhead_attn(q1, k1, v1, mask=trg_mask)\n",
    "        x = self.dropout1(x) + x\n",
    "        x = self.layer_norm1(x)\n",
    "        q2 = self.Q2(x)\n",
    "        k2 = self.K2(enc_x)\n",
    "        v2 = self.V2(enc_x)\n",
    "        o = self.mhead_attn(q2, k2, v2, mask=src_mask)\n",
    "        o = self.dropout2(o) + x\n",
    "        o = self.layer_norm2(o)\n",
    "        o = self.feed_fwd(o)\n",
    "        o = self.dropout3(o) + o\n",
    "        o = self.layer_norm3(o)\n",
    "        return o #  x: [batch_sz, seq_ln, emb_dim]\n",
    "\n",
    "# wee test\n",
    "bsize = 5\n",
    "seq_len = 3\n",
    "emb_dim = 16\n",
    "n_heads = 4\n",
    "test_x = torch.randn(bsize, seq_len, emb_dim)\n",
    "test_enc_x = torch.randn(bsize, seq_len, emb_dim)\n",
    "d1l = Decoder1L(emb_dim, n_heads)\n",
    "o = d1l(test_enc_x, test_x, src_mask=None, trg_mask=None)\n",
    "assert list(o.shape) == [5, 3, 16]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ab9b7-ec43-476a-beaa-6cff36d5ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_vocab_sz, max_seq_len, emb_dim, n_layers, n_attn_heads, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec = nn.ModuleList([Decoder1L(emb_dim, n_attn_heads) for i in range(n_layers)])\n",
    "        self.output_embeddings = nn.Embedding(out_vocab_sz, emb_dim)\n",
    "        self.fc = nn.Linear(emb_dim, emb_dim)\n",
    "        self.pos_enc = PositionalEncoder(max_seq_len, emb_dim)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        \n",
    "    def forward(self, enc_x, x, src_mask, trg_mask):\n",
    "        x = self.output_embeddings(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.dec[i](enc_x, x, src_mask, trg_mask)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# wee test\n",
    "bsize = 5\n",
    "max_seq_len = 3\n",
    "emb_dim = 16\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "out_voc = 4\n",
    "test_x = torch.randint(low=0, high=3, size=(bsize, max_seq_len))\n",
    "test_enc_x = torch.randn(bsize, seq_len, emb_dim)\n",
    "dec = Decoder(out_voc, max_seq_len, emb_dim, n_layers, n_heads)\n",
    "o = dec(test_enc_x, test_x, src_mask=None, trg_mask=None)\n",
    "assert list(o.shape) == [5, 3, 16] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a04b33-54af-43e4-95c6-5341f059f70a",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117f5c7-1ae3-4f00-929c-c12a9805379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, max_seq_len, in_vocab_sz, out_vocab_sz, emb_dim, n_layers, \n",
    "                 n_attn_heads, src_pad_idx, trg_pad_idx, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_vocab_sz, max_seq_len, emb_dim, n_layers, n_attn_heads)\n",
    "        self.decoder = Decoder(out_vocab_sz, max_seq_len, emb_dim, n_layers, n_attn_heads)\n",
    "        self.source_pad_index = src_pad_idx\n",
    "        self.target_pad_index = trg_pad_idx\n",
    "        self.n_attn_heads = n_attn_heads\n",
    "\n",
    "    def get_source_mask(self, source):\n",
    "        # true where source is NOT padding: 1 for valid tokens and 0 for padding tokens\n",
    "        src_mask = (source != self.source_pad_index).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "        return src_mask\n",
    "\n",
    "    def get_target_mask(self, target):\n",
    "        batch_size, target_len = target.size()\n",
    "    \n",
    "        # padding_mask: [batch_size, 1, 1, trg_len]\n",
    "        pad_mask = (target != self.target_pad_index).unsqueeze(1).unsqueeze(2)\n",
    "        # lookahead_mask: [1, trg_len, trg_len]\n",
    "        lookahead_mask = torch.tril(torch.ones(target_len, target_len, device=target.device)).bool()\n",
    "    \n",
    "        # Ensure both masks align with batch size and target length\n",
    "        trg_mask = pad_mask & lookahead_mask.unsqueeze(0).unsqueeze(1)\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        source_mask = self.get_source_mask(x) # [batch_size, 1, 1, source_len]\n",
    "        target_mask = self.get_target_mask(y) # [batch_size, 1, target_len, target_len]\n",
    "\n",
    "        x = self.encoder(x, source_mask)            \n",
    "        y = self.decoder(x, y, source_mask, target_mask)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1daacd-2ef4-429f-91a2-a8554b5ebc38",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5134e8e9-5710-47e8-96d8-21a6ba7a6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(en_vocab, de_vocab, pad_index):\n",
    "    # params\n",
    "    input_vocab_sz = len(en_vocab)\n",
    "    output_vocab_sz = len(de_vocab)\n",
    "    embedding_dim = 1000\n",
    "    n_layers = 2\n",
    "    n_attn_heads = 4\n",
    "    max_seq_len = 100\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # init models\n",
    "    transformer = Transformer(max_seq_len, input_vocab_sz, output_vocab_sz, embedding_dim, \n",
    "                              n_layers, n_attn_heads, de_vocab[\"<pad>\"], en_vocab[\"<pad>\"])\n",
    "\n",
    "    optimizer = optim.Adam(transformer.parameters(), lr=0.1, momentum=0.9)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "    \n",
    "    return transformer, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d14ad2-e4f1-4ca7-8428-ebd564f08712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, en_vocab, de_vocab, pad_index, n_epochs=20):\n",
    "    model, optimizer, criterion = init_model(en_vocab, de_vocab, pad_index)\n",
    "    clip = 1\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    \n",
    "    for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "        training_losses = []\n",
    "        model.train()\n",
    "        \n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            src = batch[\"en_ids\"]\n",
    "            trg = batch[\"de_ids\"]\n",
    "\n",
    "            # src shape: [batch_size, src_seq_len]\n",
    "            # trg shape: [batch_size, trg_seq_len]\n",
    "\n",
    "            # shift target: \n",
    "            # trg_input: all except the last token (for teacher forcing input)\n",
    "            # trg_output: all except the first token (for ground truth output)\n",
    "            trg_input = trg[:, :-1]  # Decoder input (teacher forcing)\n",
    "            trg_output = trg[:, 1:]  # Expected output\n",
    "\n",
    "            source_mask = model.get_source_mask(src)\n",
    "            target_mask = model.get_target_mask(trg_input)\n",
    "\n",
    "            # forward pass\n",
    "            enc_out = model.encoder(src, source_mask)\n",
    "            y_pred = model.decoder(enc_out, trg_input, source_mask, target_mask)\n",
    "\n",
    "            # adjust shapes for loss calculation\n",
    "            trg_vocab_sz = y_pred.shape[-1]\n",
    "            y_pred = y_pred.reshape(-1, trg_vocab_sz)       # [batch_size * (seq_len - 1), trg_vocab_size]\n",
    "            trg_output = trg_output.reshape(-1)             # [batch_size * (seq_len - 1)]\n",
    "\n",
    "            loss = criterion(y_pred, trg_output)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            training_losses.append(loss.item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch} average training loss: {sum(training_losses) / len(training_losses)}\")\n",
    "\n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            validation_losses = []\n",
    "\n",
    "            for i, batch in enumerate(val_dataloader):\n",
    "                src = batch[\"en_ids\"]\n",
    "                trg = batch[\"de_ids\"]\n",
    "\n",
    "                # shift target\n",
    "                trg_input = trg[:, :-1] # decoder input\n",
    "                trg_output = trg[:, 1:] # ground truth\n",
    "\n",
    "                source_mask = model.get_source_mask(src)\n",
    "                target_mask = model.get_target_mask(trg_input)\n",
    "\n",
    "                enc_out = model.encoder(src, source_mask)\n",
    "                y_pred = model.decoder(enc_out, trg_input, source_mask, target_mask)\n",
    "                trg_vocab_sz = y_pred.shape[-1]\n",
    "                y_pred = y_pred.reshape(-1, trg_vocab_sz) # [batch_size * (seq_len - 1), vocab_size]\n",
    "                trg_output = trg_output.reshape(-1) # [batch_size * (seq_len - 1)]\n",
    "\n",
    "                loss = criterion(y_pred, trg_output)\n",
    "\n",
    "                validation_losses.append(loss.item())\n",
    "\n",
    "            print(f\"Epoch {epoch} average validation loss: {sum(validation_losses) / len(validation_losses)}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4dcc4-81e4-4bbd-9611-05ff001b836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = train(train_dataloader, valid_dataloader, en_vocab, de_vocab, pad_index)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2636b-d3ea-496e-9daa-32497938c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cfed5c-5714-47d4-83a8-523d0bc5d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    pad_token=en_vocab[\"<pad>\"], \n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize and preprocess\n",
    "        tokens = [token.text.lower() for token in en_nlp.tokenizer(sentence)]\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "\n",
    "        ids = en_vocab.lookup_indices(tokens)\n",
    "        print(f\"Input tokens: {ids}\")\n",
    "\n",
    "        src_tensor = torch.LongTensor(ids).unsqueeze(0).to(device)  # shape: [1, src_len]\n",
    "\n",
    "        # generate source mask\n",
    "        source_mask = model.get_source_mask(src_tensor)\n",
    "\n",
    "        # encode input sequence\n",
    "        enc_out = model.encoder(src_tensor, source_mask)\n",
    "\n",
    "        next_token = torch.LongTensor([de_vocab[sos_token]]).unsqueeze(0).to(device)  # shape: [1, 1]\n",
    "        trg_tokens = next_token  # Autoregressive target sequence\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(max_output_length):\n",
    "            target_mask = model.get_target_mask(trg_tokens)\n",
    "            output = model.decoder(enc_out, trg_tokens, source_mask, target_mask)\n",
    "            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(0)  # Shape: [1, 1]\n",
    "            outputs.append(next_token.item())\n",
    "            if next_token.item() == de_vocab[eos_token]:\n",
    "                break\n",
    "\n",
    "            trg_tokens = torch.cat((trg_tokens, next_token), dim=1)\n",
    "\n",
    "        output_tokens = de_vocab.lookup_tokens(outputs)\n",
    "\n",
    "        print(f\"Output tokens: {outputs}\")\n",
    "        return output_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4db22-ee70-4806-b719-8c3b723111e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data\n",
    "rando_idx = np.random.randint(low=0, high=len(test_sentences))\n",
    "sentence = test_sentences[rando_idx][\"en\"]\n",
    "expected_translation = test_sentences[rando_idx][\"de\"]\n",
    "translation = translate_sentence(\n",
    "    sentence=sentence,\n",
    "    model=model,\n",
    "    en_nlp=en_nlp,\n",
    "    de_nlp=de_nlp,\n",
    "    en_vocab=en_vocab,\n",
    "    de_vocab=de_vocab,\n",
    "    sos_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "print(f\"\\nsentence: {sentence}\\n\")\n",
    "print(f\"expected_translation: {expected_translation}\\n\")\n",
    "print(f\"actual translation: {' '.join(i for i in translation[0:])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_demos",
   "language": "python",
   "name": "ml_demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
