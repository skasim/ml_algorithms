{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4f31fe-1162-4513-b18a-69367822a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm --quiet\n",
    "# ! python -m spacy download de_core_news_sm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73cdff1-cc8b-4b7c-8c43-ed9658e27d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# references: \n",
    "# https://hussainwali.medium.com/transforming-your-text-data-with-pytorch-12ec1b1c9ae6\n",
    "# https://github.com/bentrevett/pytorch-seq2seq/tree/main\n",
    "# https://adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-recurrent-neural-network-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e794a1-f25a-47bf-abaa-39ae4a3e28ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samra/anaconda3/envs/ml_demos/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import datasets\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e459ef-a4fc-4555-95b2-0b272bc1ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")\n",
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a354be-6ddf-4068-b45b-9d7cb3099e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73bff59-9f19-4a04-ad2c-3e53828b460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n",
    "    en_tokens = [token.lower() for token in en_tokens]\n",
    "    de_tokens = [token.lower() for token in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b45c10c-c88a-4db8-94da-c85a00a2741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"de_nlp\": de_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d38cbc-f286-4441-98ad-885f258dea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    "    max_tokens=1_000\n",
    ")\n",
    "\n",
    "de_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"de_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    "    max_tokens=1_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "700f8be0-39ad-4cc0-b300-b4f92191d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert en_vocab[unk_token] == de_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == de_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]\n",
    "\n",
    "en_vocab.set_default_index(unk_index)\n",
    "de_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2030c1-586f-4ba3-be7c-879072882ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, en_vocab, de_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "361b4ba5-d07b-452d-acdd-3cf2b71b7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0809c225-0fba-4770-a0b8-c4dcee8dbb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'en_ids': [2, 16, 24, 15, 25, 778, 17, 57, 80, 202, 0, 5, 3],\n",
       " 'de_ids': [2, 18, 26, 253, 30, 84, 20, 88, 7, 15, 110, 0, 0, 4, 3]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91874950-ae8f-4322-a033-0e3d98a86173",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1c2f4ff-135c-4485-ac99-c363fa634fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_ids': tensor([  2,  16,  24,  15,  25, 778,  17,  57,  80, 202,   0,   5,   3]),\n",
       " 'de_ids': tensor([  2,  18,  26, 253,  30,  84,  20,  88,   7,  15, 110,   0,   0,   4,\n",
       "           3]),\n",
       " 'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87c6512c-dc56-4c14-a855-19adcc78e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b707ce51-93b6-4dc8-b39e-a72e6bde5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6b8a7b0-3470-425d-af81-555ba50d0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_dataloader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_dataloader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8351a150-3d0e-4ae7-a03a-ecf2b27a84a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataloader)[7][\"en_ids\"].shape\n",
    "# batch is of shape seq_len, batch_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6630892-eabe-4d51-81ae-8b16dec04787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN EncDec implementation: https://arxiv.org/pdf/1406.1078"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c761143-7aa4-403a-9b28-c8a56555d27a",
   "metadata": {},
   "source": [
    "### GRU Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f57b022-b9cf-4639-8071-505c78dbd98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoderCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_size = input_dim\n",
    "        self.hidden_size = hidden_dim\n",
    "        # the hidden dims in the GRU Cell is the number of neurons in one layer. \n",
    "        # each one of those neurons processes the sentence one token at a time (at each timestep)\n",
    "        # paying attention to different parts of the sentence via the gating mechansims\n",
    "\n",
    "        self.W = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.Wz = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.Wr = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim, bias=True) # gah! this was [hidden_dim, hidden_dim] NOT [embed_dim, hidden_dim]\n",
    "        self.Uz = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.Ur = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "    def forward(self, x, h):\n",
    "        zj = torch.sigmoid(self.Wz(x) + self.Uz(h))\n",
    "        rj = torch.sigmoid(self.Wr(x) + self.Ur(h))\n",
    "        h_tilde = torch.tanh(self.W(x) + rj * self.U(h))\n",
    "        return zj * h + (1 - zj) * h_tilde\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bef87a30-266e-4f9a-abf1-593e42cebda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n",
      "Final hidden state shape: torch.Size([1, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_sz, embedding_dim, hidden_dim, n_layers=1, p_dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_sz, embedding_dim)\n",
    "        self.gru = GRUEncoderCell(embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim)\n",
    "    def forward(self, src):\n",
    "        # input is of shape [seq_len, batch_sz]\n",
    "        seq_len, batch_sz = src.shape\n",
    "        # hidden is [batch_sz, hidden_dim]\n",
    "        hidden = torch.zeros(batch_sz, self.hidden_dim) # so the entire issue was that this is batch_sz, self.hidden_dim annd NOT seq_len, batch_sz\n",
    "        # convert src tokens tensor to embeddings\n",
    "        embedded_src = self.embedding(src)\n",
    "        # embedding_src is [seq_len, batch_sz, embedding_dim]\n",
    "        for t in range(seq_len):\n",
    "            input_ = embedded_src[t, :]\n",
    "            # input_ is [batch_sz, embedding_dim]\n",
    "            # consider how this looks in the math W(x) + U(h)\n",
    "            # where x is [batch_sz, embedding_dim] and h is [batch_sz, hidden_dim]\n",
    "            hidden = self.gru(input_, hidden)\n",
    "        context = F.tanh(self.V(hidden))\n",
    "        # hidden is of shape [n_directions * n_layers, batch_sz, hidden_dim]\n",
    "        return None, context.unsqueeze(0)      \n",
    "\n",
    "# wee test\n",
    "voc_sz = 2\n",
    "b_sz = 3\n",
    "seq_l = 4\n",
    "n_lrs = 1\n",
    "x = torch.randint(0, 2, (seq_l, b_sz))\n",
    "# input is [4, 3]\n",
    "# enc = Encoder(voc_sz, b_sz, seq_l, n_lrs)\n",
    "enc = Encoder(voc_sz, b_sz, seq_l, n_lrs)\n",
    "o, h = enc(x)\n",
    "# assert list(o.shape) == [4, 3, 4]\n",
    "print(h.shape)\n",
    "assert list(h.shape) == [1, 3, 4]\n",
    "\n",
    "# Example parameters\n",
    "vocab_size = 50  # Size of vocabulary\n",
    "embedding_dim = 16  # Size of embeddings\n",
    "hidden_size = 32  # Size of hidden state\n",
    "seq_len = 10\n",
    "batch_size = 4\n",
    "\n",
    "# Random input tokens (integer indices)\n",
    "input_tokens = torch.randint(0, vocab_size, (seq_len, batch_size))\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = Encoder(vocab_size, embedding_dim, hidden_size)\n",
    "\n",
    "# Forward pass\n",
    "hidden_states, final_hidden_state = encoder(input_tokens)\n",
    "\n",
    "# print(f\"Hidden states shape: {hidden_states.shape}\")\n",
    "print(f\"Final hidden state shape: {final_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1c0d9164-97ba-4fdd-8445-04b457bcdde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUDecoderCell(nn.Module):\n",
    "    def __init__(self, factor_dim, hidden_dim, io_dim, embedding_dim, context_dim, p_dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.Oh1 = nn.Linear(hidden_dim, factor_dim, bias=True)\n",
    "        self.Oh2 = nn.Linear(factor_dim, hidden_dim * 2, bias=True) # double the hidden_dim because in max_out will choose the max of two\n",
    "        self.Oy1 = nn.Linear(embedding_dim, factor_dim, bias=True)\n",
    "        self.Oy2 = nn.Linear(factor_dim, hidden_dim * 2, bias=True)\n",
    "        self.Oc1 = nn.Linear(context_dim, factor_dim, bias=True)\n",
    "        self.Oc2 = nn.Linear(factor_dim, hidden_dim * 2, bias=True)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "    def forward(self, trg_embeddings, hidden, context):\n",
    "        # context is of shape [batch_sz, hidden_dim]\n",
    "        # hidden is of shape [batch_sz, hidden_dim]\n",
    "        # trg_embeddings is [batch_sz, embeddings_dim]\n",
    "        seq_len, batch_sz, _ = trg_embeddings.shape\n",
    "        Oh = self.Oh2(self.Oh1(hidden)) # [batch_sz, hidden_dim * 2]\n",
    "        Oy = self.Oy2(self.Oy1(trg_embeddings)) # [batch_sz, hidden_dim * 2]\n",
    "        Oc = self.Oc2(self.Oc1(context)) # [batch_sz, hidden_dim * 2]\n",
    "        # pre-activation state\n",
    "        s_prime = Oh + Oy + Oc\n",
    "        # s_prime is [batch_sz, hidden_dim * 2]\n",
    "        # activation as maxout unit\n",
    "        s_prime = s_prime.view(batch_sz, -1, 2)\n",
    "        hidden = s_prime.max(dim=-1).values\n",
    "        # hidden is [batch_sz, hidden_dim]\n",
    "        return _, hidden\n",
    "\n",
    "# wee test\n",
    "fac_dim = 2\n",
    "hid_dim = 4\n",
    "embed_dim = 5\n",
    "ctxt_dim = 4\n",
    "io_dim = 3\n",
    "sq_len = 3\n",
    "btch_sz = 5\n",
    "voc_sz = 2\n",
    "ttrg_embs = torch.randn(1, btch_sz, embed_dim)\n",
    "tctxt = torch.randn(btch_sz, ctxt_dim)\n",
    "thidden = torch.randn(btch_sz, hid_dim)\n",
    "\n",
    "test_dec = GRUDecoderCell(fac_dim, hid_dim, io_dim, embed_dim, ctxt_dim)\n",
    "o, h = test_dec(ttrg_embs, thidden, tctxt)\n",
    "assert list(h.shape) == [btch_sz, hid_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c1b2de3f-89e8-4ff7-94fa-c094d4a1dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOneStep(nn.Module):\n",
    "    def __init__(self, input_output_dim, factor_dim, embedding_dim, hidden_dim, context_dim, n_layers, p_dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.input_output_dim = input_output_dim\n",
    "        self.embedding = nn.Embedding(input_output_dim, embedding_dim)\n",
    "        self.gru = GRUDecoderCell(factor_dim, hidden_dim, io_dim, embedding_dim, context_dim, p_dropout)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        # need to include this to project output to vocab, i.e., have total number of vocab to do argmax over\n",
    "        self.fc = nn.Linear(hidden_dim, input_output_dim)\n",
    "    def forward(self, trg, hidden, context):\n",
    "        # since we get only one token at a time, \n",
    "        # it is of shape [batch_sz]\n",
    "        # so we need to unsqueeze to make it [1, batch_sz]\n",
    "        trg = trg.unsqueeze(0) # we changed what it is expecting so maybe not unsqueeze\n",
    "        embeddings = self.dropout(self.embedding(trg))\n",
    "        _, hidden = self.gru(embeddings, hidden, context)\n",
    "        # hidden is of shape [n_directions * n_layers, batch_sz, hidden_dim]\n",
    "        # but input for fc has to be squeezed at 0-th dim to remove the dimension added earlier\n",
    "        # so that shape of out is [batch_sz, hidden_dim]\n",
    "        # compute output logits\n",
    "        out = self.fc(hidden)\n",
    "        # out here is [batch_sz, input_output_dim]\n",
    "        # so here you turn hidden to vocab for output for predictions\n",
    "        # and continue to use hidden to further process the tokens\n",
    "        return out.squeeze(0), hidden.unsqueeze(0)\n",
    "\n",
    "\n",
    "# wee test\n",
    "io_dim = 5\n",
    "fac_dim = 2\n",
    "emb_dim = 10\n",
    "h_dim = 4\n",
    "ctxt_dim = 4\n",
    "n_lrs = 1\n",
    "voc_sz = 3\n",
    "d1s = DecoderOneStep(io_dim, fac_dim, emb_dim, h_dim, ctxt_dim, n_lrs) \n",
    "single_step_input = torch.tensor([1,2,2,1])\n",
    "h = torch.randn(4, h_dim) # batch_sz is 4 because the input into single step is [seq_len, batch_sz]\n",
    "ctxt = torch.randn(4, ctxt_dim)\n",
    "o1s, h1s = d1s(single_step_input, h, ctxt)\n",
    "assert list(o1s.shape) == [4, 5]\n",
    "assert list(h1s.shape) == [1, 4, 4] # should this be [1, 4, 4] ??????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "644148dc-978b-48f6-9fbf-a30faf46794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, one_step_decoder, hidden_emb):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_emb\n",
    "        self.one_step_decoder = one_step_decoder\n",
    "        self.V = nn.Linear(hidden_emb, hidden_emb, bias=True)\n",
    "    def forward(self, trg, context, teacher_forcing_ratio=0.5):\n",
    "        # context is shape [n_directions * n_layers, batch_sz, hidden_dim]\n",
    "        seq_len, batch_sz = trg.shape\n",
    "        vocab_sz = self.one_step_decoder.input_output_dim\n",
    "        # hidden = torch.tanh(self.V(context))\n",
    "        hidden = torch.zeros(batch_sz, self.hidden_dim)\n",
    "        predictions = torch.zeros(seq_len, batch_sz, vocab_sz)\n",
    "        # predictions [seq_len, batch_sz, vocab_sz]\n",
    "        input_ = trg[0, :]\n",
    "        for t in range(1, seq_len):\n",
    "            # input_ is [batch_sz]\n",
    "            pred, hidden = self.one_step_decoder(input_, hidden, context) \n",
    "            # out [batch_sz, input_output_dim]\n",
    "            # hidden [n_directions * n_layers, batch_sz, hidden_dim]\n",
    "            predictions[t] = pred\n",
    "            is_teacher_force =  random.random() < teacher_forcing_ratio \n",
    "            input_ = trg[t] if is_teacher_force else pred.argmax(dim=-1)\n",
    "        # predictions are of shape [seq_len, batch_sz, input_output_dim]\n",
    "        # predictions = torch.stack(predictions)\n",
    "        return predictions\n",
    "\n",
    "# wee test\n",
    "io_dim = 5\n",
    "emb_dim = 10\n",
    "h_dim = 4\n",
    "n_lrs = 1\n",
    "d1s = DecoderOneStep(io_dim, fac_dim, emb_dim, h_dim, ctxt_dim, n_lrs) \n",
    "h = torch.randn(1, 4, 4) # [n_directions * n_layers, batch_sz, hidden_dim]\n",
    "b_sz = 4\n",
    "seq_l = 5\n",
    "dec = Decoder(d1s, h_dim)\n",
    "y = torch.randint(0, 2, (seq_l, b_sz))\n",
    "odec = dec(y, h)\n",
    "assert list(odec.shape) == [seq_l, b_sz, io_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6edd0544-30f1-4d53-aa2a-3d5d557c5f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDec(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        _, context = self.encoder(src)\n",
    "        predictions = self.decoder(trg, context, teacher_forcing_ratio)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5134e8e9-5710-47e8-96d8-21a6ba7a6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(en_vocab, de_vocab, pad_index):\n",
    "    # params\n",
    "    input_dim = len(en_vocab)\n",
    "    output_dim = len(de_vocab)\n",
    "    encoder_embedding_dim = 256\n",
    "    decoder_embedding_dim = 256\n",
    "    factorized_dim = 64\n",
    "    hidden_dim = 512\n",
    "    context_dim = 512\n",
    "    encoder_dropout = 0.5\n",
    "    decoder_dropout = 0.5\n",
    "    n_layers = 1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # init models\n",
    "    encoder = Encoder(input_dim, encoder_embedding_dim, hidden_dim, n_layers, encoder_dropout)\n",
    "    decoder_one_step = DecoderOneStep(output_dim, factorized_dim, decoder_embedding_dim, hidden_dim, context_dim, n_layers, decoder_dropout)\n",
    "    decoder = Decoder(decoder_one_step, hidden_dim)\n",
    "    seq2seq = EncDec(encoder, decoder)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.Adam(seq2seq.parameters())\n",
    "    \n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "    \n",
    "    return seq2seq, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f6d14ad2-e4f1-4ca7-8428-ebd564f08712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, en_vocab, de_vocab, pad_index, n_epochs=3):\n",
    "    model, optimizer, criterion = init_model(en_vocab, de_vocab, pad_index)\n",
    "    clip = 1\n",
    "    # training\n",
    "    for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "        training_losses = []\n",
    "        model.train()\n",
    "        \n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            src = batch[\"en_ids\"]\n",
    "            trg = batch[\"de_ids\"]\n",
    "            # src shape [src_seq_len, batch_sz]\n",
    "            # trg shape [trg_seq_len, batch_sz]\n",
    "\n",
    "            # forward pass\n",
    "            y_pred = model(src, trg, teacher_forcing_ratio=0.5)\n",
    "            # y_pred shape [trg_seq_len, batch_sz, trg_vocab_sz]\n",
    "            trg_vocab_sz = y_pred.shape[-1]\n",
    "            # trg_vocab_sz shape is just len of trg vocab\n",
    "            # discard first token from output\n",
    "            y_pred = y_pred[1:].view(-1, trg_vocab_sz) # means do whatever you want with other dims and last dim has to be trg_vocab_sz\n",
    "            # or y_pred after discarding first token \n",
    "            # shape is [trg_seq_len - 1, batch_sz, vocab_sz]\n",
    "            # .view(-1, trg_vocab_sz returns shape\n",
    "            # [(trg_seq_len - 1) * batch_sz, vocab_sz]\n",
    "            # so now trg needs to be updated same as above\n",
    "            trg = trg[1:].view(-1) # not sure why this can be [(trg_seq_len - 1) * batch_sz]\n",
    "\n",
    "            # calc loss\n",
    "            loss = criterion(y_pred, trg)\n",
    "\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            training_losses.append(loss.item())\n",
    "        print(f\"epoch {epoch} average training loss: {sum(training_losses) / len(training_losses)}\")\n",
    "    \n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            validation_losses = []\n",
    "            for i, batch in enumerate(val_dataloader):\n",
    "                src = batch[\"en_ids\"]\n",
    "                trg = batch[\"de_ids\"]\n",
    "    \n",
    "                # forward pass\n",
    "                y_pred = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "                trg_vocab_sz = y_pred.shape[-1]\n",
    "                y_pred = y_pred[1:].view(-1, trg_vocab_sz)\n",
    "                trg = trg[1:].view(-1)\n",
    "    \n",
    "                # calc loss\n",
    "                loss = criterion(y_pred, trg)\n",
    "    \n",
    "                validation_losses.append(loss.item())\n",
    "            print(f\"epoch {epoch} average validation loss: {sum(validation_losses) / len(validation_losses)}\")   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "05e4dcc4-81e4-4bbd-9611-05ff001b836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 average training loss: 3.9202741358248674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████                            | 1/3 [02:59<05:58, 179.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 average validation loss: 3.854813516139984\n",
      "epoch 1 average training loss: 3.0957113866764017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████              | 2/3 [05:42<02:49, 169.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 average validation loss: 3.337374657392502\n",
      "epoch 2 average training loss: 2.672259381164013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 3/3 [08:23<00:00, 167.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 average validation loss: 3.1824993193149567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncDec(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(998, 256)\n",
       "    (gru): GRUEncoderCell(\n",
       "      (W): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (Wz): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (Wr): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (U): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Uz): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Ur): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (V): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (one_step_decoder): DecoderOneStep(\n",
       "      (embedding): Embedding(998, 256)\n",
       "      (gru): GRUDecoderCell(\n",
       "        (Oh1): Linear(in_features=512, out_features=64, bias=True)\n",
       "        (Oh2): Linear(in_features=64, out_features=1024, bias=True)\n",
       "        (Oy1): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (Oy2): Linear(in_features=64, out_features=1024, bias=True)\n",
       "        (Oc1): Linear(in_features=512, out_features=64, bias=True)\n",
       "        (Oc2): Linear(in_features=64, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (fc): Linear(in_features=512, out_features=998, bias=True)\n",
       "    )\n",
       "    (V): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model = train(train_dataloader, valid_dataloader, en_vocab, de_vocab, pad_index)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d2d2636b-d3ea-496e-9daa-32497938c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,012,774 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f0cfed5c-5714-47d4-83a8-523d0bc5d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        tokens = [token.text for token in en_nlp.tokenizer(sentence)]\n",
    "        print(tokens)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        ids = en_vocab.lookup_indices(tokens)\n",
    "        print(f\"in tokens: {ids}\")\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        _, context = model.encoder(tensor)\n",
    "        # hidden = context\n",
    "        hidden =  torch.zeros(1, 512) # batch_sz of 1 because only one example for inference and hididen dim of 512 as in model training\n",
    "        next_token = de_vocab.lookup_indices([sos_token])\n",
    "        next_token = next_token[0]\n",
    "        outputs = []\n",
    "        for _ in range(max_output_length):\n",
    "            next_token = torch.LongTensor([next_token])\n",
    "            output, hidden = model.decoder.one_step_decoder(next_token, hidden, context)\n",
    "            next_token = output.argmax(dim=-1).item()\n",
    "            if next_token == de_vocab[eos_token]:\n",
    "                break\n",
    "            else:\n",
    "                outputs.append(next_token)\n",
    "        print(f\"out tokens: {outputs}\")\n",
    "        tokens = de_vocab.lookup_tokens(outputs)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "88c4db22-ee70-4806-b719-8c3b723111e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Several', 'Asian', 'men', 'wearing', 'black', 'clothing', 'in', 'some', 'kind', 'of', 'station', '.']\n",
      "in tokens: [2, 113, 106, 30, 22, 26, 219, 6, 74, 958, 12, 481, 5, 3]\n",
      "out tokens: [105, 315, 30, 7, 478, 883, 7, 6, 0, 4]\n",
      "\n",
      "sentence: Several Asian men wearing black clothing in some kind of station.\n",
      "\n",
      "expected_translation: Mehrere asiatische Männer in schwarzer Kleidung in einer Art Station.\n",
      "\n",
      "actual translation: vier asiatische männer in uniform anzügen in einem <unk> .\n"
     ]
    }
   ],
   "source": [
    "test_sentences = test_data\n",
    "rando_idx = np.random.randint(low=0, high=len(test_sentences))\n",
    "sentence = test_sentences[rando_idx][\"en\"]\n",
    "expected_translation = test_sentences[rando_idx][\"de\"]\n",
    "translation = translate_sentence(\n",
    "    sentence=sentence,\n",
    "    model=model,\n",
    "    en_nlp=en_nlp,\n",
    "    de_nlp=de_nlp,\n",
    "    en_vocab=en_vocab,\n",
    "    de_vocab=de_vocab,\n",
    "    sos_token=\"<sos>\",\n",
    "    \n",
    "    eos_token=\"<eos>\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "print(f\"\\nsentence: {sentence}\\n\")\n",
    "print(f\"expected_translation: {expected_translation}\\n\")\n",
    "print(f\"actual translation: {' '.join(i for i in translation[0:])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_demos",
   "language": "python",
   "name": "ml_demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
