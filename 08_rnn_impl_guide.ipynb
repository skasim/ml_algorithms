{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be14deb-63aa-491c-bf70-1d95e3d16a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f73e81-54a4-4003-a2a4-5c6453e532fa",
   "metadata": {},
   "source": [
    "# Multi-Layer RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f9bfac3-390c-4f52-b2fe-e658efaac50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz):\n",
    "        super().__init__()\n",
    "        self.i2h = nn.Linear(input_sz, hidden_sz)\n",
    "        self.h2h = nn.Linear(hidden_sz, hidden_sz)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        hx = self.i2h(x)\n",
    "        hh = self.h2h(h_prev)\n",
    "        h = torch.tanh(hx + hh)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac378719-d09c-41b0-9366-37bc7bccc280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a multi-layer RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.rnn = nn.ModuleList([RNNCell(input_sz, hidden_sz) if i == 0 else RNNCell(hidden_sz, hidden_sz)\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_sz, seq_len, _ = x.shape\n",
    "        h = [torch.zeros(batch_sz, self.hidden_sz)for _ in range(n_layers)]\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            input_t = x[:, t, :]\n",
    "            for layer in range(self.n_layers):\n",
    "                h[layer] = self.rnn[layer](input_t, h[layer])\n",
    "                input_t = h[layer] # output of current layer is input for next layer\n",
    "            outputs.append(h[-1].unsqueeze(1)) # store last layer output\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs, h[-1] # outputs and final hidden state\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94680030-9eaf-4471-af0d-32f3e6eb53ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 4])\n",
      "Final hidden state: tensor([[-0.6623,  0.7879,  0.6665,  0.4409],\n",
      "        [-0.0420,  0.8941,  0.6779,  0.4650]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "input_sz = 3 # size of input features\n",
    "hidden_sz = 4 # size of hidden state\n",
    "n_layers = 2 # number of layers of rnn cells\n",
    "batch_sz = 2\n",
    "seq_len = 5\n",
    "\n",
    "# batch of input data (batch_size = 2, seq_length = 5, input_size = 3)\n",
    "x = torch.randn(batch_sz, seq_len, input_sz)\n",
    "\n",
    "# forward pass through RNN\n",
    "rnn_model = RNN(input_sz, hidden_sz, n_layers)\n",
    "output, h_final = rnn_model(x)\n",
    "print(f\"Output shape: {output.shape}\")  # (2, 5, 4)\n",
    "print(f\"Final hidden state: {h_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d18790-1f9e-4a90-bc4d-4e2a5f73934b",
   "metadata": {},
   "source": [
    "# GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10ce05b0-c1df-45e4-b7e6-3400d28b2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz):\n",
    "        super().__init__()\n",
    "        self.Wr = nn.Linear(input_sz, hidden_sz, bias=True)\n",
    "        self.Ur = nn.Linear(hidden_sz, hidden_sz, bias=False)\n",
    "        self.Wz = nn.Linear(input_sz, hidden_sz, bias=True)\n",
    "        self.Uz = nn.Linear(hidden_sz, hidden_sz, bias=False)\n",
    "        self.Wh = nn.Linear(input_sz, hidden_sz, bias=True)\n",
    "        self.Uh = nn.Linear(hidden_sz, hidden_sz, bias=False)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        # compute reset gate\n",
    "        r_x = self.Wr(x)\n",
    "        r_h = self.Ur(h_prev)\n",
    "        r = torch.sigmoid(r_x + r_h)\n",
    "\n",
    "        # compute hidden gate\n",
    "        z_x = self.Wz(x)\n",
    "        z_h = self.Uz(h_prev)\n",
    "        z = torch.sigmoid(z_x + z_h)\n",
    "\n",
    "        # compute candidate hidden state\n",
    "        h_x = self.Wh(x)\n",
    "        h_h = self.Uh(h_prev)\n",
    "        # with the candidate hidden (h_tilde) when rest gate is close to 0, previous hidden state is ignored\n",
    "        # and cadidate_hidden is reset with h_x (or current input)\n",
    "        h_tilde = torch.tanh(h_x + (r * h_h)) # different from paper's implementation but how pytorch does it\n",
    "\n",
    "        # compute final hidden state\n",
    "        # below update gate determines how much of candiate_hidden vs previous hidden state to return as new hidden state\n",
    "        h = (z * h_prev) + ((1 - z) * h_tilde)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "089ccc89-6623-4240-9951-c161c77674d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz, n_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.ModuleList([GRUCell(input_sz, hidden_sz) if i == 0 else GRUCell(hidden_sz, hidden_sz)\n",
    "            for i in range(self.n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_sz, seq_len, _ = x.shape\n",
    "        h = [torch.zeros(batch_sz, self.hidden_sz) for _ in range(self.n_layers)]\n",
    "        # Process the input sequence step-by-step\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            input_t = x[:, t, :]  # Extract input at time t\n",
    "\n",
    "            # Pass through each layer's GRU cell\n",
    "            for layer in range(self.n_layers):\n",
    "                h[layer] = self.gru[layer](input_t, h[layer])\n",
    "                input_t = h[layer]  # Output of this layer becomes input for the next\n",
    "\n",
    "            outputs.append(h[-1].unsqueeze(1))  # Store output for this step\n",
    "\n",
    "        # Concatenate all outputs along the sequence dimension\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8725ff31-0e02-42c2-9f49-b9245bdd2ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 5])\n",
      "Final hidden state: tensor([[-0.6623,  0.7879,  0.6665,  0.4409],\n",
      "        [-0.0420,  0.8941,  0.6779,  0.4650]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# input params\n",
    "input_sz = 3 # size of features in input\n",
    "hidden_sz = 5 # size of hidden states\n",
    "seq_len = 4 # size of the sequence\n",
    "n_layers = 2 # number of gru layers\n",
    "batch_sz = 2 # number of sequences in a batch\n",
    "\n",
    "# create input of (batch_sz, seq_len, input_sz)\n",
    "x = torch.randn(batch_sz, seq_len, input_sz)\n",
    "\n",
    "gru_model = GRU(input_sz, hidden_sz, n_layers)\n",
    "\n",
    "# one forward pass\n",
    "output = gru_model(x)\n",
    "\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")  #  Should print: (2, 4, 5)\n",
    "print(f\"Final hidden state: {h_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c74a7-f84c-452c-8671-aedf155136bf",
   "metadata": {},
   "source": [
    "# Enocder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2147a667-57fa-412a-a034-c6c733edca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz, n_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.ModuleList([GRUCell(input_sz, hidden_sz) if i == 0 else GRUCell(hidden_sz, hidden_sz)\n",
    "            for i in range(self.n_layers)\n",
    "        ])\n",
    "        self.V = nn.Linear(self.hidden_sz, self.hidden_sz)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_sz, seq_len, _ = x.shape\n",
    "        h = [torch.zeros(batch_sz, self.hidden_sz) for _ in range(self.n_layers)]\n",
    "        # Process the input sequence step-by-step\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            input_t = x[:, t, :]  # Extract input at time t\n",
    "\n",
    "            # Pass through each layer's GRU cell\n",
    "            for layer in range(self.n_layers):\n",
    "                h[layer] = self.gru[layer](input_t, h[layer])\n",
    "                input_t = h[layer]  # Output of this layer becomes input for the next\n",
    "\n",
    "            outputs.append(h[-1].unsqueeze(1))  # Store output for this step\n",
    "\n",
    "        Vh = self.V(h[-1].unsqueeze(1))\n",
    "        # c is the representation of the source phrase. It is computed with the hidden state at the N step\n",
    "        # i.e., at the end of the source phrase\n",
    "        c = torch.tanh(Vh)\n",
    "        \n",
    "        return c, h[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9d90892-1381-473f-b47e-f266c3ed6e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUDencoder(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz, n_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.ModuleList([GRUCell(input_sz, hidden_sz) if i == 0 else GRUCell(hidden_sz, hidden_sz)\n",
    "            for i in range(self.n_layers)\n",
    "        ])\n",
    "        self.V = nn.Linear(self.hidden_sz, self.hidden_sz)\n",
    "\n",
    "    def forward(self, y, h):\n",
    "        batch_sz, seq_len, _ = y.shape\n",
    "        Vc = self.V(h)\n",
    "        h_prime = torch.tanh(Vc)\n",
    "        # Process the input sequence step-by-step\n",
    "        all_logits = [] # prob of generating the next word\n",
    "        for t in range(seq_length):\n",
    "            input_t = y[:, t, :]  # Extract input at time t\n",
    "\n",
    "            # Pass through each layer's GRU cell\n",
    "            for layer in range(self.n_layers):\n",
    "                h_prime[layer] = self.gru[layer](input_t, h_prime[layer])\n",
    "                input_t = h_prime[layer]  # Output of this layer becomes input for the next\n",
    "                raw_logits = h_prime[-1].unsqueeze(1)\n",
    "                raw_logits = torch.relu(raw_logits) # use this forn ow instead of maxout\n",
    "\n",
    "            all_logits.append(row_logits)  # Store output for this step\n",
    "        logits = torch.vstack(all_logits, dim=1)\n",
    "        probabilities = torch.softmax(logits)\n",
    "        return probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_demos",
   "language": "python",
   "name": "ml_demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
